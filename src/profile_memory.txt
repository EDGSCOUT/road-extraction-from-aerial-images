Line #    Mem usage    Increment   Line Contents
================================================
    86  132.531 MiB    0.000 MiB   @profile
    87                             def main(argv=None):  # pylint: disable=unused-argument
    88                             
    89                                 # Start profiling
    90  132.531 MiB    0.000 MiB       pr = cProfile.Profile()
    91  132.531 MiB    0.000 MiB       pr.enable()
    92                             
    93  132.531 MiB    0.000 MiB       initialization_check()
    94                                 ######################
    95                                 ### INITIALIZATION ###
    96                                 ######################
    97  132.535 MiB    0.004 MiB       print("----------- SETTINGS -----------")
    98  132.535 MiB    0.000 MiB       print("Batch size: " + str(BATCH_SIZE))
    99  132.535 MiB    0.000 MiB       print("Context size: " + str(const.IMG_CONTEXT_SIZE))
   100  132.535 MiB    0.000 MiB       print("Patch size: " + str(const.IMG_PATCH_SIZE))
   101  132.539 MiB    0.004 MiB       print("Time is termination criterion: " + str(TERMINATE_AFTER_TIME))
   102  132.539 MiB    0.000 MiB       print("Train for: " + str(MAX_TRAINING_TIME_IN_SEC) + "s")
   103  132.539 MiB    0.000 MiB       print("--------------------------------\n")
   104  132.539 MiB    0.000 MiB       np.random.seed(NP_SEED)
   105  132.543 MiB    0.004 MiB       num_epochs = NUM_EPOCHS
   106                             
   107  132.543 MiB    0.000 MiB       train_data_filename = ROOT_DIR + "data/training/images/"
   108  132.543 MiB    0.000 MiB       train_labels_filename = ROOT_DIR + "data/training/groundtruth/"
   109  132.543 MiB    0.000 MiB       test_data_filename = ROOT_DIR + "data/test_set/"
   110                             
   111                                 #######################
   112                                 ### LOADING PATCHES ###
   113                                 #######################
   114  132.543 MiB    0.000 MiB       prefix = ROOT_DIR + "objects/"
   115  132.543 MiB    0.000 MiB       patches_filename = prefix + "patches_imgs"
   116  132.543 MiB    0.000 MiB       labels_filename = prefix + "patches_labels"
   117  132.543 MiB    0.000 MiB       patches_balanced_filename = prefix + "patches_imgs_balanced"
   118  132.543 MiB    0.000 MiB       labels_balanced_filename = prefix + "patches_labels_balanced"
   119                             
   120  132.543 MiB    0.000 MiB       if IMG_PATCHES_RESTORE and os.path.isfile(patches_balanced_filename + ".npy"):
   121  132.543 MiB    0.000 MiB           if BALANCE_SIZE_OF_CLASSES:
   122 4126.895 MiB 3994.352 MiB               train_data = np.load(patches_balanced_filename + ".npy")
   123 4126.910 MiB    0.016 MiB               train_labels = np.load(labels_balanced_filename + ".npy")
   124                                     else:
   125                                         train_data = np.load(patches_filename + ".npy")
   126                                         train_labels = np.load(labels_filename + ".npy")
   127 4126.910 MiB    0.000 MiB           train_size = train_labels.shape[0]
   128                                 else:
   129                                     if os.path.isfile(const.PATCHES_MEAN_PATH + ".npy"):
   130                                         os.remove(const.PATCHES_MEAN_PATH + ".npy")
   131                                     print(const.PATCHES_MEAN_PATH + ".npy" + " removed.")
   132                                     train_data = dlm.extract_data(train_data_filename, TRAINING_SIZE, 1)
   133                                     train_labels = dlm.extract_labels(train_labels_filename, TRAINING_SIZE, 1)
   134                                     np.save(patches_filename, train_data)
   135                                     np.save(labels_filename, train_labels)
   136                             
   137 4126.910 MiB    0.000 MiB       print("Shape of patches: " + str(train_data.shape))
   138 4126.910 MiB    0.000 MiB       print("Shape of labels:  " + str(train_labels.shape))
   139                             
   140                                 ##############################
   141                                 ### BALANCING TRAINING SET ###
   142                                 ##############################
   143 4126.910 MiB    0.000 MiB       if BALANCE_SIZE_OF_CLASSES:
   144                                     ### AUXILIARY FUNCTION ###
   145 4126.910 MiB    0.000 MiB           def num_of_datapoints_per_class():
   146 4126.910 MiB    0.000 MiB               c0 = 0
   147 4126.910 MiB    0.000 MiB               c1 = 0
   148 4126.910 MiB    0.000 MiB               for i in range(len(train_labels)):
   149 4126.910 MiB    0.000 MiB                   if train_labels[i][0] == 1:
   150 4126.910 MiB    0.000 MiB                       c0 = c0 + 1
   151                                             else:
   152                                                 c1 = c1 + 1
   153 4126.871 MiB   -0.039 MiB               print("Number of data points per class: c0 = " + str(c0) + " c1 = " + str(c1))
   154 4126.871 MiB    0.000 MiB               return (c0, c1)
   155                             
   156                                     ### END OF AUXILIARY FUNCTION ###
   157                             
   158                                     # Computing per class number of data points
   159 4126.871 MiB    0.000 MiB           (c0, c1) = num_of_datapoints_per_class();
   160                             
   161                                     # Balancing
   162 4126.871 MiB    0.000 MiB           if IMG_PATCHES_RESTORE:
   163 4126.871 MiB    0.000 MiB               print("Skipping balancing - balanced data already loaded from the disk.")
   164                                     else:
   165                                         print("Balancing training data.")
   166                                         min_c = min(c0, c1)
   167                                         idx0 = [i for i, j in enumerate(train_labels) if j[0] == 1]
   168                                         idx1 = [i for i, j in enumerate(train_labels) if j[1] == 1]
   169                                         new_indices = idx0[0:min_c] + idx1[0:min_c]
   170                             
   171                                         train_data = train_data[new_indices, :, :, :]
   172                                         train_labels = train_labels[new_indices]
   173                                         train_size = train_labels.shape[0]
   174                             
   175                                         num_of_datapoints_per_class();
   176                                         np.save(patches_balanced_filename, train_data)
   177                                         np.save(labels_balanced_filename, train_labels)
   178                             
   179                                 ##########################################
   180                                 ### SETUP OUT OF SAMPLE VALIDATION SET ###
   181                                 ##########################################
   182 4126.871 MiB    0.000 MiB       PATCHES_VALIDATION = ROOT_DIR + "objects/validation_patches"
   183 4126.871 MiB    0.000 MiB       LABELS_VALIDATION = ROOT_DIR + "objects/validation_labels"
   184 4126.871 MiB    0.000 MiB       os.path.isfile(const.PATCHES_MEAN_PATH + ".npy")
   185                             
   186 4126.871 MiB    0.000 MiB       if VALIDATE:
   187                                     if RESTORE_MODEL and os.path.isfile(PATCHES_VALIDATION + ".npy") and os.path.isfile(LABELS_VALIDATION + ".npy"):
   188                                         msg = "Validation data read from the disk."
   189                                         validation_data = np.load(PATCHES_VALIDATION + ".npy")
   190                                         validation_labels = np.load(LABELS_VALIDATION + ".npy")
   191                                     else:
   192                                         msg = "Validation data recreated from training data."
   193                                         perm_indices = np.random.permutation(np.arange(0, len(train_data)))
   194                             
   195                                         validation_data = train_data[perm_indices[0:VALIDATION_SIZE]]
   196                                         validation_labels = train_labels[perm_indices[0:VALIDATION_SIZE]]
   197                             
   198                                         np.save(PATCHES_VALIDATION, validation_data)
   199                                         np.save(LABELS_VALIDATION, validation_labels)
   200                             
   201                                         train_data = train_data[perm_indices[VALIDATION_SIZE:perm_indices.shape[0]]]
   202                                         train_labels = train_labels[perm_indices[VALIDATION_SIZE:perm_indices.shape[0]]]
   203                                         train_size = train_labels.shape[0]
   204                             
   205                                     print("\n----------- VALIDATION INFO -----------")
   206                                     print(msg)
   207                                     print("Shape of validation set: " + str(validation_data.shape))
   208                                     print("New shape of training set: " + str(train_data.shape))
   209                                     print("New shape of labels set: " + str(train_labels.shape))
   210                                     print("---------------------------------------\n")
   211                             
   212                                 ####################################
   213                                 ### CREATING VARIABLES FOR GRAPH ###
   214                                 ####################################
   215 4126.871 MiB    0.000 MiB       train_data_node = tf.placeholder(tf.float32,
   216 4126.973 MiB    0.102 MiB                                        shape=(BATCH_SIZE, const.IMG_CONTEXT_SIZE, const.IMG_CONTEXT_SIZE, NUM_CHANNELS))
   217 4126.996 MiB    0.023 MiB       train_labels_node = tf.placeholder(tf.float32, shape=(BATCH_SIZE, NUM_LABELS))
   218                             
   219                                 ###############
   220                                 ### WEIGHTS ###
   221                                 ###############
   222 4126.996 MiB    0.000 MiB       num_of_CNN_params_to_learn = 0
   223 4126.996 MiB    0.000 MiB       num_of_FC_params_to_learn = 0
   224                             
   225                                 ### CONVOLUTIONAL LAYER 1 ###
   226 4126.996 MiB    0.000 MiB       with tf.name_scope('conv1') as scope:
   227 4126.996 MiB    0.000 MiB           conv1_dim = 5
   228 4126.996 MiB    0.000 MiB           conv1_num_of_maps = 16
   229 4126.996 MiB    0.000 MiB           conv1_weights = tf.Variable(
   230 4126.996 MiB    0.000 MiB               tf.truncated_normal([conv1_dim, conv1_dim, NUM_CHANNELS, conv1_num_of_maps],
   231 4126.996 MiB    0.000 MiB                                   stddev=0.1,
   232 4127.164 MiB    0.168 MiB                                   seed=SEED), name='weights')
   233 4127.219 MiB    0.055 MiB           conv1_biases = tf.Variable(tf.zeros([conv1_num_of_maps]), name='biases')
   234 4127.219 MiB    0.000 MiB       num_of_CNN_params_to_learn += conv1_dim * conv1_dim * conv1_num_of_maps
   235                             
   236                                 ### CONVOLUTIONAL LAYER 2 ###
   237 4127.219 MiB    0.000 MiB       with tf.name_scope('conv2') as scope:
   238 4127.219 MiB    0.000 MiB           conv2_dim = 3
   239 4127.219 MiB    0.000 MiB           conv2_num_of_maps = 32
   240 4127.219 MiB    0.000 MiB           conv2_weights = tf.Variable(
   241 4127.219 MiB    0.000 MiB               tf.truncated_normal([conv2_dim, conv2_dim, conv1_num_of_maps, conv2_num_of_maps],
   242 4127.219 MiB    0.000 MiB                                   stddev=0.1,
   243 4127.301 MiB    0.082 MiB                                   seed=SEED), name='weights')
   244 4127.344 MiB    0.043 MiB           conv2_biases = tf.Variable(tf.constant(0.1, shape=[conv2_num_of_maps]), name='biases')
   245 4127.344 MiB    0.000 MiB       num_of_CNN_params_to_learn += conv2_dim * conv2_dim * conv2_num_of_maps
   246                             
   247                                 ### CONVOLUTIONAL LAYER 3 ###
   248 4127.344 MiB    0.000 MiB       with tf.name_scope('conv3') as scope:
   249 4127.344 MiB    0.000 MiB           conv3_dim = 3
   250 4127.344 MiB    0.000 MiB           conv3_num_of_maps = 32
   251 4127.344 MiB    0.000 MiB           conv3_weights = tf.Variable(
   252 4127.344 MiB    0.000 MiB               tf.truncated_normal([conv3_dim, conv3_dim, conv2_num_of_maps, conv3_num_of_maps],
   253 4127.344 MiB    0.000 MiB                                   stddev=0.1,
   254 4127.410 MiB    0.066 MiB                                   seed=SEED), name='weights')
   255 4127.457 MiB    0.047 MiB           conv3_biases = tf.Variable(tf.constant(0.1, shape=[conv3_num_of_maps]), name='biases')
   256 4127.457 MiB    0.000 MiB       num_of_CNN_params_to_learn += conv3_dim * conv3_dim * conv3_num_of_maps
   257                             
   258                                 ### CONVOLUTIONAL LAYER 4 ###
   259 4127.457 MiB    0.000 MiB       with tf.name_scope('conv4') as scope:
   260 4127.457 MiB    0.000 MiB           conv4_dim = 3
   261 4127.457 MiB    0.000 MiB           conv4_num_of_maps = 64
   262 4127.457 MiB    0.000 MiB           conv4_weights = tf.Variable(
   263 4127.457 MiB    0.000 MiB               tf.truncated_normal([conv4_dim, conv4_dim, conv3_num_of_maps, conv4_num_of_maps],
   264 4127.457 MiB    0.000 MiB                                   stddev=0.1,
   265 4127.527 MiB    0.070 MiB                                   seed=SEED), name='weights')
   266 4127.566 MiB    0.039 MiB           conv4_biases = tf.Variable(tf.constant(0.1, shape=[conv4_num_of_maps]), name='biases')
   267 4127.566 MiB    0.000 MiB       num_of_CNN_params_to_learn += conv4_dim * conv4_dim * conv4_num_of_maps
   268                             
   269                                 ### FULLY CONNECTED LAYER 1 ###
   270 4127.566 MiB    0.000 MiB       tmp_neuron_num = int((const.IMG_PATCH_SIZE / 8) * (const.IMG_PATCH_SIZE / 8) * conv4_num_of_maps);
   271 4127.566 MiB    0.000 MiB       with tf.name_scope('fc1') as scope:
   272 4127.566 MiB    0.000 MiB           fc1_size = 64
   273 4127.566 MiB    0.000 MiB           fc1_weights = tf.Variable(
   274 4127.566 MiB    0.000 MiB               tf.truncated_normal([tmp_neuron_num, fc1_size],
   275 4127.566 MiB    0.000 MiB                                   stddev=0.1,
   276 4127.637 MiB    0.070 MiB                                   seed=SEED), name='weights')
   277 4127.680 MiB    0.043 MiB           fc1_biases = tf.Variable(tf.constant(0.1, shape=[fc1_size]), name='biases')
   278 4127.680 MiB    0.000 MiB       num_of_FC_params_to_learn += tmp_neuron_num * fc1_size;
   279                             
   280                                 ### FULLY CONNECTED LAYER 2 ###
   281 4127.680 MiB    0.000 MiB       with tf.name_scope('fc1') as scope:
   282 4127.680 MiB    0.000 MiB           fc2_weights = tf.Variable(
   283 4127.680 MiB    0.000 MiB               tf.truncated_normal([fc1_size, NUM_LABELS],
   284 4127.680 MiB    0.000 MiB                                   stddev=0.1,
   285 4127.766 MiB    0.086 MiB                                   seed=SEED), name='weights')
   286 4127.785 MiB    0.020 MiB           fc2_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS]), name='biases')
   287 4127.785 MiB    0.000 MiB       num_of_FC_params_to_learn += fc1_size * NUM_LABELS
   288                             
   289 4127.785 MiB    0.000 MiB       if not RESTORE_MODEL:
   290                                     print("----------- NUM OF PARAMS TO LEARN -----------")
   291                                     print("Num of CNN params to learn: " + str(num_of_CNN_params_to_learn));
   292                                     print("Num of FC params to learn: " + str(num_of_FC_params_to_learn));
   293                                     print("Total num of params to learn: " + str(num_of_CNN_params_to_learn + num_of_FC_params_to_learn))
   294                                     print("----------------------------------------------\n")
   295                             
   296                                 # Get prediction for given input image
   297 5544.492 MiB 1416.707 MiB       def get_prediction(tf_session, img, stride):
   298 5544.492 MiB    0.000 MiB           data = pem.zero_center(
   299 5619.684 MiB   75.191 MiB               np.asarray(pem.input_img_crop(img, const.IMG_PATCH_SIZE, const.IMG_BORDER_SIZE, stride, 0)))
   300 5653.531 MiB   33.848 MiB           data_node = tf.cast(tf.constant(data), tf.float32)
   301 5864.234 MiB  210.703 MiB           prediction = tf_session.run(tf.nn.softmax(model(data_node)))
   302                             
   303                                     ### UPSAMPLING ###
   304 5864.234 MiB    0.000 MiB           imgheight = img.shape[0]
   305 5864.234 MiB    0.000 MiB           imgwidth = img.shape[1]
   306 5864.234 MiB    0.000 MiB           prediction_img_per_pixel = np.zeros((imgheight, imgwidth))
   307 5864.234 MiB    0.000 MiB           count_per_pixel = np.zeros((imgheight, imgwidth))
   308 5864.234 MiB    0.000 MiB           idx = 0
   309 5867.055 MiB    2.820 MiB           for i in range(0, imgheight - const.IMG_PATCH_SIZE + 1, stride):
   310 5867.055 MiB    0.000 MiB               for j in range(0, imgwidth - const.IMG_PATCH_SIZE + 1, stride):
   311 5867.055 MiB    0.000 MiB                   prediction_img_per_pixel[j: j + const.IMG_PATCH_SIZE, i: i + const.IMG_PATCH_SIZE] += prediction[idx][1]
   312 5867.055 MiB    0.000 MiB                   count_per_pixel[j: j + const.IMG_PATCH_SIZE, i: i + const.IMG_PATCH_SIZE] += 1.0
   313 5867.055 MiB    0.000 MiB                   idx += 1
   314                             
   315 5867.055 MiB    0.000 MiB           prediction = np.zeros((imgheight * imgwidth, 2))
   316 5867.055 MiB    0.000 MiB           idx = 0
   317 5867.055 MiB    0.000 MiB           for i in range(imgheight):
   318 5867.055 MiB    0.000 MiB               for j in range(imgwidth):
   319 5867.055 MiB    0.000 MiB                   prediction[idx][1] = prediction_img_per_pixel[j][i] / count_per_pixel[j][i]
   320 5867.055 MiB    0.000 MiB                   prediction[idx][0] = 1.0 - prediction[idx][1]
   321 5867.055 MiB    0.000 MiB                   idx += 1
   322                                     ### END OF UPSAMPLING ###
   323                             
   324 5867.055 MiB    0.000 MiB           return prediction
   325                             
   326                                 # Get prediction overlaid on the original image for given input file
   327 5544.492 MiB -322.562 MiB       def get_prediction_with_overlay(tf_session, input_path, output_path_overlay, output_path_raw,
   328 4127.785 MiB -1416.707 MiB                                       truth_input_path=None):
   329                                     ### AUXILIARY FUNCTION 0 ###
   330 5776.805 MiB 1649.020 MiB           def label_to_binary_img(imgwidth, imgheight, w, h, labels):
   331 5776.805 MiB    0.000 MiB               array_labels = np.zeros([imgwidth, imgheight])
   332 5776.805 MiB    0.000 MiB               idx = 0
   333 5779.625 MiB    2.820 MiB               for i in range(0, imgheight, h):
   334 5779.625 MiB    0.000 MiB                   for j in range(0, imgwidth, w):
   335 5779.625 MiB    0.000 MiB                       if labels[idx][0] > 0.5:
   336 5779.625 MiB    0.000 MiB                           l = 0
   337                                                 else:
   338                                                     l = 1
   339 5779.625 MiB    0.000 MiB                       array_labels[j:j + w, i:i + h] = l
   340 5779.625 MiB    0.000 MiB                       idx = idx + 1
   341 5779.625 MiB    0.000 MiB               return array_labels
   342                             
   343                                     ### END OF AUXILIARY FUNCTION 0 ###
   344                             
   345                                     ### AUXILIARY FUNCTION 1 ###
   346 5776.805 MiB   -2.820 MiB           def label_to_img(imgwidth, imgheight, w, h, labels):
   347 5776.805 MiB    0.000 MiB               array_labels = np.zeros([imgwidth, imgheight])
   348 5776.805 MiB    0.000 MiB               idx = 0
   349 5776.805 MiB    0.000 MiB               for i in range(0, imgheight, h):
   350 5776.805 MiB    0.000 MiB                   for j in range(0, imgwidth, w):
   351 5776.805 MiB    0.000 MiB                       array_labels[j:j + w, i:i + h] = labels[idx][1]
   352 5776.805 MiB    0.000 MiB                       idx = idx + 1
   353 5776.805 MiB    0.000 MiB               return array_labels
   354                                         ### END OF AUXILIARY FUNCTION 1 ###
   355                             
   356                                     ### AUXILIARY FUNCTION 2 ###
   357 5782.445 MiB    5.641 MiB           def make_img_overlay(img, predicted_img, true_img=None):
   358 5782.445 MiB    0.000 MiB               from PIL import Image  # TODO import here because not available on Euler
   359                                         ### AUXILIARY FUNCTION 2.1 ###
   360 5785.266 MiB    2.820 MiB               def img_float_to_uint8(img):
   361 5785.266 MiB    0.000 MiB                   rimg = img - np.min(img)
   362 5790.555 MiB    5.289 MiB                   rimg = (rimg / np.max(rimg) * PIXEL_DEPTH).round().astype(np.uint8)
   363 5790.555 MiB    0.000 MiB                   return rimg
   364                             
   365                                         ### END OF AUXILIARY FUNCTION 2.1 ###
   366                             
   367 5782.445 MiB   -8.109 MiB               w = img.shape[0]
   368 5782.445 MiB    0.000 MiB               h = img.shape[1]
   369 5782.445 MiB    0.000 MiB               color_mask = np.zeros((w, h, 3), dtype=np.uint8)
   370 5785.266 MiB    2.820 MiB               color_mask[:, :, 0] = predicted_img * PIXEL_DEPTH
   371 5785.266 MiB    0.000 MiB               if (true_img is not None):
   372                                             color_mask[:, :, 1] = true_img * PIXEL_DEPTH
   373                             
   374 5790.555 MiB    5.289 MiB               img8 = img_float_to_uint8(img)
   375 5793.375 MiB    2.820 MiB               background = Image.fromarray(img8, 'RGB').convert("RGBA")
   376 5794.785 MiB    1.410 MiB               overlay = Image.fromarray(color_mask, 'RGB').convert("RGBA")
   377 5794.785 MiB    0.000 MiB               new_img = Image.blend(background, overlay, 0.2)
   378 5794.785 MiB    0.000 MiB               return new_img
   379                             
   380                                     ### END OF AUXILIARY FUNCTION 2 ###
   381                             
   382 5779.625 MiB  -15.160 MiB           def pixels_to_patches(img, round=False, foreground_threshold=0.5, stride=const.IMG_PATCH_SIZE):
   383 5779.625 MiB    0.000 MiB               res_img = np.zeros(img.shape)
   384 5782.445 MiB    2.820 MiB               for i in range(0, img.shape[0], stride):
   385 5782.445 MiB    0.000 MiB                   for j in range(0, img.shape[1], stride):
   386 5782.445 MiB    0.000 MiB                       tmp = np.zeros((stride, stride))
   387 5782.445 MiB    0.000 MiB                       tmp[0: stride, 0: stride] = img[j: j + stride, i: i + stride]
   388 5782.445 MiB    0.000 MiB                       tmp[tmp < 0.5] = 0
   389 5782.445 MiB    0.000 MiB                       tmp[tmp >= 0.5] = 1
   390 5782.445 MiB    0.000 MiB                       res_img[j: j + stride, i: i + stride] = np.mean(tmp)
   391                             
   392                                                 # res_img[j : j + stride, i : i + stride] = np.mean(img[j : j + stride, i : i + stride])
   393 5782.445 MiB    0.000 MiB                       if round:
   394 5782.445 MiB    0.000 MiB                           if res_img[j, i] >= foreground_threshold:
   395                                                         res_img[j: j + stride, i: i + stride] = 1
   396                                                     else:
   397 5782.445 MiB    0.000 MiB                               res_img[j: j + stride, i: i + stride] = 0
   398 5782.445 MiB    0.000 MiB               return res_img
   399                             
   400                                     # Read images from disk
   401 5544.492 MiB -237.953 MiB           img = mpimg.imread(input_path)
   402 5544.492 MiB    0.000 MiB           img_truth = None
   403 5544.492 MiB    0.000 MiB           if truth_input_path != None:
   404                                         img_truth = mpimg.imread(truth_input_path)
   405                             
   406                                     # Get prediction
   407 5544.492 MiB    0.000 MiB           stride = const.IMG_PATCH_SIZE # TODO 4
   408 5776.805 MiB  232.312 MiB           prediction = get_prediction(tf_session, img, stride)
   409                                     ### POST PROCESSING ###
   410                                     # for i in range(1):
   411                                     #     prediction = pm.postprocess_prediction(prediction, int(np.sqrt(prediction.shape[0])), int(np.sqrt(prediction.shape[0])))
   412                                     #######################
   413                             
   414                                     # Show per pixel probabilities
   415 5776.805 MiB    0.000 MiB           prediction_as_per_pixel_img = label_to_img(img.shape[0], img.shape[1], 1, 1, prediction)
   416                             
   417                                     # Show per patch probabilities
   418 5776.805 MiB    0.000 MiB           prediction_as_img = pixels_to_patches(prediction_as_per_pixel_img)
   419                             
   420                                     # Rounded to 0 / 1 - per pixel
   421 5779.625 MiB    2.820 MiB           prediction_as_binary_per_pixel_img = label_to_binary_img(img.shape[0], img.shape[1], 1, 1, prediction)
   422                             
   423                                     # Round to 0 / 1 - per patch
   424 5782.445 MiB    2.820 MiB           prediction_as_binary_img = pixels_to_patches(prediction_as_per_pixel_img, True)
   425                             
   426                                     # Save output to disk
   427                                     # Overlay
   428 5715.574 MiB  -66.871 MiB           oimg = make_img_overlay(img, prediction_as_binary_per_pixel_img, img_truth)
   429 5625.574 MiB  -90.000 MiB           oimg.save(output_path_overlay + "_pixels.png")
   430                             
   431 5626.984 MiB    1.410 MiB           oimg2 = make_img_overlay(img, prediction_as_binary_img, img_truth)
   432 5626.984 MiB    0.000 MiB           oimg2.save(output_path_overlay + "_patches.png")
   433                             
   434                                     # Raw image
   435 5616.059 MiB  -10.926 MiB           scipy.misc.imsave(output_path_raw + "_pixels.png", prediction_as_per_pixel_img)
   436 5616.059 MiB    0.000 MiB           scipy.misc.imsave(output_path_raw + "_patches.png", prediction_as_img)
   437                             
   438 5616.059 MiB    0.000 MiB           return (prediction, prediction_as_binary_img)
   439                             
   440 4127.785 MiB -1488.273 MiB       def validate(validation_model, labels):
   441                                     print("\n --- Validation ---")
   442                                     prediction = s.run(tf.nn.softmax(validation_model))
   443                                     err = error_rate(prediction, labels)
   444                                     print("Validation set size: %d" % VALIDATION_SIZE)
   445                                     print("Error: %.1f%%" % err)
   446                                     print("--------------------")
   447                                     return err
   448                             
   449 5653.531 MiB 1525.746 MiB       def model(data, train=False):
   450                                     # CONV. LAYER 1
   451 5653.531 MiB    0.000 MiB           conv1 = tf.nn.conv2d(data, conv1_weights, strides=[1, 1, 1, 1], padding='SAME')
   452 5653.531 MiB    0.000 MiB           relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))
   453 5653.531 MiB    0.000 MiB           norm1 = tf.nn.lrn(relu1)
   454 5653.531 MiB    0.000 MiB           pool1 = tf.nn.max_pool(norm1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
   455                             
   456                                     # CONV. LAYER 2
   457 5653.531 MiB    0.000 MiB           conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1, 1, 1, 1], padding='SAME')
   458 5653.531 MiB    0.000 MiB           relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))
   459 5653.531 MiB    0.000 MiB           norm2 = tf.nn.lrn(relu2)
   460 5653.531 MiB    0.000 MiB           pool2 = tf.nn.max_pool(norm2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
   461                             
   462                                     # CONV. LAYER 3
   463 5653.531 MiB    0.000 MiB           conv3 = tf.nn.conv2d(pool2, conv3_weights, strides=[1, 1, 1, 1], padding='SAME')
   464 5653.531 MiB    0.000 MiB           relu3 = tf.nn.relu(tf.nn.bias_add(conv3, conv3_biases))
   465 5653.531 MiB    0.000 MiB           norm3 = tf.nn.lrn(relu3)
   466 5653.531 MiB    0.000 MiB           pool3 = tf.nn.max_pool(norm3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
   467                             
   468                                     # CONV. LAYER 4
   469 5653.531 MiB    0.000 MiB           conv4 = tf.nn.conv2d(pool3, conv4_weights, strides=[1, 1, 1, 1], padding='SAME')
   470 5653.531 MiB    0.000 MiB           relu4 = tf.nn.relu(tf.nn.bias_add(conv4, conv4_biases))
   471 5653.531 MiB    0.000 MiB           norm4 = tf.nn.lrn(relu4)
   472 5653.531 MiB    0.000 MiB           pool4 = tf.nn.max_pool(norm4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
   473 5653.531 MiB    0.000 MiB           conv_out = pool4
   474                             
   475                                     # Reshape the feature map cuboid into a 2D matrix to feed it to the fully connected layers.
   476 5653.531 MiB    0.000 MiB           pool_shape = conv_out.get_shape().as_list()
   477 5653.531 MiB    0.000 MiB           reshape = tf.reshape(
   478 5653.531 MiB    0.000 MiB               conv_out,
   479 5653.531 MiB    0.000 MiB               [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])
   480                             
   481                                     # Fully connected layer. Note that the '+' operation automatically broadcasts the biases.
   482 5653.531 MiB    0.000 MiB           hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)
   483                             
   484                                     ##### DROPOUT #####
   485                                     # if train:
   486                                     #     hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)
   487                             
   488                                     ### FINAL ACTIVATION ###
   489 5653.531 MiB    0.000 MiB           out = tf.sigmoid(tf.matmul(hidden, fc2_weights) + fc2_biases)
   490                             
   491                                     # Make an image summary for 4d tensor image with index idx
   492 5653.531 MiB    0.000 MiB           def get_image_summary(img, idx=0):
   493 4129.594 MiB -1523.938 MiB               V = tf.slice(img, (0, 0, 0, idx), (1, -1, -1, 1))
   494 4129.594 MiB    0.000 MiB               img_w = img.get_shape().as_list()[1]
   495 4129.594 MiB    0.000 MiB               img_h = img.get_shape().as_list()[2]
   496 4129.629 MiB    0.035 MiB               min_value = tf.reduce_min(V)
   497 4129.637 MiB    0.008 MiB               V = V - min_value
   498 4129.672 MiB    0.035 MiB               max_value = tf.reduce_max(V)
   499 4129.691 MiB    0.020 MiB               V = V / (max_value * PIXEL_DEPTH)
   500 4129.715 MiB    0.023 MiB               V = tf.reshape(V, (img_w, img_h, 1))
   501 4129.727 MiB    0.012 MiB               V = tf.transpose(V, (2, 0, 1))
   502 4129.738 MiB    0.012 MiB               V = tf.reshape(V, (-1, img_w, img_h, 1))
   503 4129.738 MiB    0.000 MiB               return V
   504                             
   505 5653.531 MiB 1523.793 MiB           if train == True:
   506 4128.273 MiB -1525.258 MiB               tf.image_summary('summary_data', get_image_summary(data))
   507 4128.457 MiB    0.184 MiB               tf.image_summary('summary_conv1', get_image_summary(conv1))
   508 4128.652 MiB    0.195 MiB               tf.image_summary('summary_pool1', get_image_summary(pool1))
   509 4128.844 MiB    0.191 MiB               tf.image_summary('summary_conv2', get_image_summary(conv2))
   510 4129.023 MiB    0.180 MiB               tf.image_summary('summary_pool2', get_image_summary(pool2))
   511 4129.199 MiB    0.176 MiB               tf.image_summary('summary_conv3', get_image_summary(conv3))
   512 4129.391 MiB    0.191 MiB               tf.image_summary('summary_pool3', get_image_summary(pool3))
   513 4129.570 MiB    0.180 MiB               tf.image_summary('summary_conv4', get_image_summary(conv4))
   514 4129.766 MiB    0.195 MiB               tf.image_summary('summary_pool4', get_image_summary(pool4))
   515 4129.777 MiB    0.012 MiB               tf.histogram_summary('weights_conv1', conv1_weights)
   516 4129.805 MiB    0.027 MiB               tf.histogram_summary('weights_conv2', conv2_weights)
   517 4129.816 MiB    0.012 MiB               tf.histogram_summary('weights_conv3', conv3_weights)
   518 4129.836 MiB    0.020 MiB               tf.histogram_summary('weights_conv4', conv4_weights)
   519 4129.895 MiB    0.059 MiB               tf.histogram_summary('weights_FC1', fc1_weights)
   520 4129.910 MiB    0.016 MiB               tf.histogram_summary('weights_FC2', fc2_weights)
   521                             
   522 5653.531 MiB 1523.621 MiB           return out
   523                             
   524                                 ### END OF MODEL ###
   525                             
   526                                 ##################
   527                                 ### SETUP LOSS ###
   528                                 ##################
   529 4129.910 MiB -1523.621 MiB       logits = model(train_data_node, True)
   530 4129.945 MiB    0.035 MiB       loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, train_labels_node))
   531 4129.969 MiB    0.023 MiB       tf.scalar_summary('loss', loss)
   532                             
   533 4130.012 MiB    0.043 MiB       cumulative_loss = tf.Variable(1.0)
   534 4130.012 MiB    0.000 MiB       loss_window = np.zeros(LOSS_WINDOW_SIZE)
   535 4130.012 MiB    0.000 MiB       index_loss_window = 0
   536                             
   537 4130.027 MiB    0.016 MiB       tf.scalar_summary('loss_smoothed', cumulative_loss)
   538                                 #########################
   539                                 ### L2 REGULARIZATION ###
   540                                 #########################
   541                                 # regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +
   542                                 #                 tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))
   543                                 # loss = tf.add(loss, 5e-4 * regularizers)
   544                             
   545                                 ### IN SAMPLE ERROR ###
   546 4130.059 MiB    0.031 MiB       error_insample_tensor = tf.Variable(0)
   547 4130.066 MiB    0.008 MiB       tf.scalar_summary('error_insample_smoothed', error_insample_tensor)
   548                             
   549 4130.066 MiB    0.000 MiB       insample_error_window = np.zeros(LOSS_WINDOW_SIZE)
   550 4130.066 MiB    0.000 MiB       index_insample_error_window = 0
   551                             
   552                                 ### VALIDATION ERROR ###
   553 4130.098 MiB    0.031 MiB       error_validation_tensor = tf.Variable(0)
   554 4130.121 MiB    0.023 MiB       tf.scalar_summary('error_validation', error_validation_tensor)
   555                             
   556                                 # Create the validation model here to prevent recreating large constant nodes in graph later
   557 4130.121 MiB    0.000 MiB       if VALIDATE:
   558                                     data_node = tf.cast(tf.constant(np.asarray(validation_data)), tf.float32)
   559                                     validation_model = model(data_node)
   560                             
   561                                 ### SUMMARY OF WEIGHTS ###
   562 4130.121 MiB    0.000 MiB       all_params_node = [conv1_weights, conv1_biases, conv2_weights, conv2_biases, conv3_weights, conv3_biases,
   563 4130.121 MiB    0.000 MiB                          fc1_weights, fc1_biases, fc2_weights, fc2_biases]
   564 4130.121 MiB    0.000 MiB       all_params_names = ['conv1_weights', 'conv1_biases', 'conv2_weights', 'conv2_biases', 'conv3_weights',
   565 4130.121 MiB    0.000 MiB                           'conv3_biases', 'fc1_weights', 'fc1_biases', 'fc2_weights', 'fc2_biases']
   566 4130.863 MiB    0.742 MiB       all_grads_node = tf.gradients(loss, all_params_node)
   567 4130.863 MiB    0.000 MiB       all_grad_norms_node = []
   568 4131.680 MiB    0.816 MiB       for i in range(0, len(all_grads_node)):
   569 4131.660 MiB   -0.020 MiB           norm_grad_i = tf.global_norm([all_grads_node[i]])
   570 4131.660 MiB    0.000 MiB           all_grad_norms_node.append(norm_grad_i)
   571 4131.680 MiB    0.020 MiB           tf.scalar_summary(all_params_names[i], norm_grad_i)
   572                             
   573                                 #######################
   574                                 ### OPTIMIZER SETUP ###
   575                                 #######################
   576 4131.707 MiB    0.027 MiB       batch = tf.Variable(0)
   577 4131.742 MiB    0.035 MiB       learning_rate = tf.Variable(BASE_LEARNING_RATE)
   578                                 # tf.train.exponential_decay(
   579                                 #     BASE_LEARNING_RATE,  # Base learning rate.
   580                                 #     batch * BATCH_SIZE,  # Current index into the dataset.
   581                                 #     DECAY_STEP,          # Decay step.
   582                                 #     DECAY_RATE,          # Decay rate.
   583                                 #     staircase=True)
   584                             
   585 4131.770 MiB    0.027 MiB       tf.scalar_summary('learning_rate', learning_rate)
   586                             
   587                                 # optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(loss, global_step=batch)
   588 4134.148 MiB    2.379 MiB       optimizer = tf.train.AdamOptimizer(learning_rate, epsilon=0.1).minimize(loss, global_step=batch)
   589                             
   590                                 ### Predictions for the minibatch, validation set and test set. ###
   591 4134.148 MiB    0.000 MiB       train_prediction = tf.nn.softmax(logits)
   592                             
   593                                 # Add ops to save and restore all the variables.
   594 4135.723 MiB    1.574 MiB       saver = tf.train.Saver()
   595                             
   596                                 #######################
   597                                 ### RUNNING SESSION ###
   598                                 #######################
   599 4135.949 MiB    0.227 MiB       with tf.Session() as s:
   600 4135.949 MiB    0.000 MiB           if RESTORE_MODEL:
   601 4147.242 MiB   11.293 MiB               saver.restore(s, FLAGS.train_dir + "/model.ckpt")
   602 4147.242 MiB    0.000 MiB               print("### MODEL RESTORED ###")
   603                                     else:
   604                                         tf.initialize_all_variables().run()
   605                             
   606                                         # Build the summary operation based on the TF collection of Summaries.
   607                                         summary_op = tf.merge_all_summaries()
   608                                         summary_writer = tf.train.SummaryWriter(FLAGS.train_dir, graph=s.graph)
   609                                         print("### MODEL INITIALIZED ###")
   610                                         print("### TRAINING STARTED ###")
   611                             
   612                                         training_indices = range(train_size)
   613                                         start = time.time()
   614                                         run_training = True
   615                                         iepoch = 0
   616                                         batch_index = 1;
   617                                         while run_training:
   618                                             perm_indices = np.random.permutation(training_indices)
   619                             
   620                                             for step in range(int(train_size / BATCH_SIZE)):
   621                                                 if not run_training:
   622                                                     break;
   623                             
   624                                                 offset = (batch_index * BATCH_SIZE) % (train_size - BATCH_SIZE)
   625                                                 batch_indices = perm_indices[offset:(offset + BATCH_SIZE)]
   626                             
   627                                                 # Compute the offset of the current minibatch in the data.
   628                                                 # Note that we could use better randomization across epochs.
   629                                                 batch_data = train_data[batch_indices, :, :, :]
   630                                                 batch_labels = train_labels[batch_indices]
   631                             
   632                                                 # This dictionary maps the batch data (as a np array) to the node in the graph is should be fed to.
   633                                                 feed_dict = {train_data_node: batch_data,
   634                                                              train_labels_node: batch_labels}
   635                             
   636                                                 # Run the operations
   637                                                 _, l, lr, predictions = s.run(
   638                                                     [optimizer, loss, learning_rate, train_prediction],
   639                                                     feed_dict=feed_dict)
   640                             
   641                                                 # Update cumulative loss
   642                                                 loss_window[index_loss_window] = l
   643                                                 index_loss_window = (index_loss_window + 1) % loss_window.shape[0]
   644                             
   645                                                 # Update insample error
   646                                                 insample_error_window[index_insample_error_window] = error_rate(predictions, batch_labels)
   647                                                 index_insample_error_window = (index_insample_error_window + 1) % insample_error_window.shape[0]
   648                             
   649                                                 if batch_index % RECORDING_STEP == 0 and batch_index > 0:
   650                                                     closs = np.mean(loss_window)
   651                                                     s.run(cumulative_loss.assign(closs))
   652                             
   653                                                     insample_error = error_rate(predictions, batch_labels)
   654                                                     s.run(error_insample_tensor.assign(insample_error))
   655                             
   656                                                     if VALIDATE and batch_index % VALIDATION_STEP == 0:
   657                                                         validation_err = validate(validation_model, validation_labels)
   658                                                         s.run(error_validation_tensor.assign(validation_err))
   659                             
   660                                                     # Writing to disk
   661                                                     summary_str = s.run(summary_op, feed_dict=feed_dict)
   662                                                     summary_writer.add_summary(summary_str, batch_index)
   663                                                     summary_writer.flush()
   664                             
   665                                                     print("\nEpoch: %d, Batch #: %d" % (iepoch, step))
   666                                                     print("Global step:     %d" % (batch_index * BATCH_SIZE))
   667                                                     print("Time elapsed:    %.3fs" % (time.time() - start))
   668                                                     print("Minibatch loss:  %.6f" % l)
   669                                                     print("Cumulative loss: %.6f" % closs)
   670                                                     print("Learning rate:   %.6f" % lr)
   671                             
   672                                                     print("Minibatch insample error: %.1f%%" % insample_error)
   673                                                     sys.stdout.flush()
   674                             
   675                                                     # Save the variables to disk.
   676                                                     save_path = saver.save(s, FLAGS.train_dir + "/model.ckpt")
   677                             
   678                                                 batch_index += 1
   679                                                 if (TERMINATE_AFTER_TIME and time.time() - start > MAX_TRAINING_TIME_IN_SEC):
   680                                                     run_training = False;
   681                                                     # Save the variables to disk.
   682                                                     save_path = saver.save(s, FLAGS.train_dir + "/model.ckpt")
   683                             
   684                                             iepoch += 1
   685                                             if (not TERMINATE_AFTER_TIME and iepoch >= NUM_EPOCHS):
   686                                                 run_training = False;
   687                                                 # Save the variables to disk.
   688                                                 save_path = saver.save(s, FLAGS.train_dir + "/model.ckpt")
   689                             
   690 4147.242 MiB    0.000 MiB           prefix_results = ROOT_DIR + "results/"
   691                             
   692 4147.242 MiB    0.000 MiB           if VISUALIZE_PREDICTION_ON_TRAINING_SET:
   693                                         print("--- Visualizing prediction on training set ---")
   694                                         prediction_training_dir = prefix_results + "predictions_training/"
   695                                         if not os.path.isdir(prediction_training_dir):
   696                                             os.mkdir(prediction_training_dir)
   697                                         limit = TRAINING_SIZE + 1 if VISUALIZE_NUM == -1 else VISUALIZE_NUM
   698                                         for i in range(1, limit):
   699                                             print("Image: " + str(i))
   700                                             img_name = "satImage_%.3d" % i
   701                                             input_path = train_data_filename + img_name + ".png"
   702                                             truth_path = train_labels_filename + img_name + ".png"
   703                                             output_path_overlay = prediction_training_dir + "overlay_" + img_name
   704                                             output_path_raw = prediction_training_dir + "raw_" + img_name
   705                             
   706                                             get_prediction_with_overlay(s, input_path, output_path_overlay, output_path_raw, truth_path)
   707                             
   708 4147.242 MiB    0.000 MiB           if VALIDATE:
   709                                         validation_err = validate(validation_model, validation_labels)
   710                             
   711 4147.242 MiB    0.000 MiB           if RUN_ON_TEST_SET:
   712 4147.242 MiB    0.000 MiB               print("--- Running prediction on test set ---")
   713 4147.242 MiB    0.000 MiB               prediction_test_dir = prefix_results + "predictions_test/"
   714 4147.242 MiB    0.000 MiB               if not os.path.isdir(prediction_test_dir):
   715                                             os.mkdir(prediction_test_dir)
   716                             
   717 4147.242 MiB    0.000 MiB               with open(prefix_results + "submission.csv", "w") as csvfile:
   718 4147.242 MiB    0.000 MiB                   writer = csv.writer(csvfile, delimiter=',')
   719 4147.246 MiB    0.004 MiB                   writer.writerow(['id', 'prediction'])
   720 5544.492 MiB 1397.246 MiB                   for i in range(1, TEST_SIZE + 1):
   721                             
   722 5544.492 MiB    0.000 MiB                       print("Test img: " + str(i))
   723                                                 # Visualization
   724 5544.492 MiB    0.000 MiB                       img_name = "test_" + str(i)
   725 5544.492 MiB    0.000 MiB                       input_path = test_data_filename + img_name + ".png"
   726 5544.492 MiB    0.000 MiB                       output_path_overlay = prediction_test_dir + "overlay_" + img_name
   727 5544.492 MiB    0.000 MiB                       output_path_raw = prediction_test_dir + "raw_" + img_name
   728                             
   729 5544.492 MiB    0.000 MiB                       (_, prediction_as_img) = get_prediction_with_overlay(s, input_path, output_path_overlay,
   730 5544.492 MiB    0.000 MiB                                                                            output_path_raw)
   731 5544.492 MiB    0.000 MiB                       prediction_as_img = prediction_as_img.astype(np.int)
   732                             
   733                                                 # Saving to csv file for submission
   734 5544.492 MiB    0.000 MiB                       num_rows = prediction_as_img.shape[0]
   735 5544.492 MiB    0.000 MiB                       num_cols = prediction_as_img.shape[1]
   736                                                 #rows_out = np.empty((0, 2))
   737 5544.492 MiB    0.000 MiB                       for x in range(0, num_rows, const.IMG_PATCH_SIZE):
   738 5544.492 MiB    0.000 MiB                           for y in range(0, num_cols, const.IMG_PATCH_SIZE):
   739 5544.492 MiB    0.000 MiB                               id = str(i).zfill(3) + "_" + str(x) + "_" + str(y)
   740                                                         #next_row = np.array([[id, str(prediction_as_img[y][x])]])
   741 5544.492 MiB    0.000 MiB                               writer.writerow([id, str(prediction_as_img[y][x])])
   742                                                         #rows_out = np.concatenate((rows_out, next_row))
   743                                                 #writer.writerows(rows_out)
   744                             
   745                                 # End profiling and save stats
   746 5419.094 MiB -125.398 MiB       pr.disable()
   747 1948.109 MiB -3470.984 MiB       s = io.StringIO()
   748 1948.109 MiB    0.000 MiB       sortby = 'cumulative'
   749 1948.109 MiB    0.000 MiB       stream = open('profile.txt', 'w');
   750 1949.141 MiB    1.031 MiB       ps = pstats.Stats(pr, stream=stream).sort_stats(sortby)
   751 1949.141 MiB    0.000 MiB       ps.print_stats()